{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16,2746)\n",
      "(16,2900)\n",
      "TestingTest dimension74610000\n",
      "23-Oct 00:39:50:INFO:root:Constructing net TweetDetect-train on Mocha.CPUBackend...\n",
      "23-Oct 00:39:50:INFO:root:Topological sorting 8 layers...\n",
      "23-Oct 00:39:50:INFO:root:Setup layers...\n",
      "23-Oct 00:39:50:INFO:root:Network constructed!\n",
      "23-Oct 00:39:50:INFO:root:Constructing net TweetDetect-train on Mocha.CPUBackend...\n"
     ]
    },
    {
     "ename": "LoadError",
     "evalue": "LoadError: UndefVarError: ConfusionMatrixLayer not defined\nwhile loading In[6], in expression starting on line 170",
     "output_type": "error",
     "traceback": [
      "LoadError: UndefVarError: ConfusionMatrixLayer not defined\nwhile loading In[6], in expression starting on line 170",
      "",
      " in MLLearnNeuralNet at In[6]:129"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23-Oct 00:39:50:INFO:root:Topological sorting 8 layers...\n",
      "23-Oct 00:39:50:INFO:root:Setup layers...\n",
      "23-Oct 00:39:50:DEBUG:root:InnerProductLayer(ip1): sharing weights and bias\n",
      "23-Oct 00:39:50:DEBUG:root:InnerProductLayer(ip2): sharing weights and bias\n",
      "23-Oct 00:39:50:DEBUG:root:InnerProductLayer(ip3): sharing weights and bias\n",
      "23-Oct 00:39:50:DEBUG:root:InnerProductLayer(ip4): sharing weights and bias\n",
      "23-Oct 00:39:50:DEBUG:root:InnerProductLayer(ip5): sharing weights and bias\n",
      "23-Oct 00:39:50:DEBUG:root:InnerProductLayer(ip6): sharing weights and bias\n",
      "23-Oct 00:39:50:INFO:root:Network constructed!\n"
     ]
    }
   ],
   "source": [
    "#Do ML by picking up data from automated conversion.\n",
    "#Utilizes achen12/Mocha.jl fork.\n",
    "using JLD\n",
    "using Mocha\n",
    "using PyPlot\n",
    "\n",
    "function  setupVectorMultiClass(x,y)\n",
    "    #Find balance Sets of labels\n",
    "    histo_y_train = hist(y_train,32)\n",
    "    minSetCount = findmin(histo_y_train[2])[1]\n",
    "    #maxSetCount = findmax(histo_y_train[2])[1]\n",
    "    idy = [ find(x -> x == i,y_train)[1:minSetCount]  for i in  collect(histo_y_train[1])[2:end]]\n",
    "    return reduce(vcat,idy)\n",
    "end\n",
    "\n",
    "function normalizeByTrendNoTrend(x)\n",
    "    #Baseline Normalization\n",
    "    x = x ./ sum(x,1)\n",
    "\n",
    "    #spikeNormalization\n",
    "    x = abs(x - vcat(zeros(1,size(x,2)),x[1:(size(x,1)-1),:]))\n",
    "    alpha = 1.2\n",
    "    x = x .^  alpha\n",
    "\n",
    "    #Smoothing\n",
    "    x = cumsum(x,1)\n",
    "    return x\n",
    "end\n",
    "\n",
    "function  setupTrainingVectorBinary(x,y)\n",
    "    #Find balance Sets of labels\n",
    "    #idySetPos = vcat(find(i-> (i== 30) || (i == 29),y))\n",
    "    idySetPos = vcat(find(i-> (i.>15),y))\n",
    "    idySetNeg = rand(vcat(find(i-> (i .==0),y)),length(idySetPos))\n",
    "    idy = vcat(idySetPos, idySetNeg) \n",
    "    shuffle!(idy)\n",
    "    #Select Features\n",
    "    x1 = x[1:6:96,idy] \n",
    "    x1 = normalizeByTrendNoTrend(x1)\n",
    "    #x2 = x[2:6:96,idy] ./ 2^14\n",
    "    x = x1#vcat(x1,x2)\n",
    "    #x = log(log(x+10))\n",
    "    y = y[idy]\n",
    "    y = y .> 15\n",
    "    #Normalize X-Vector\n",
    "    println(size(x))\n",
    "    #x = vcat(x ./ sum(x,1),log(findmax(x,1)[1])) #.- 0.5;\n",
    "    #x = x ./ findmax(x,1)[1] #.- 0.5;\n",
    "    return (Array{Float64,2}(full(x)),Array{Float64,1}(full(y)[:,1]))\n",
    "end\n",
    "\n",
    "function  setupVectorBinary(x,y)\n",
    "    #Select Features\n",
    "    x1 = x[1:6:96,:] \n",
    "    x1 = normalizeByTrendNoTrend(x1)\n",
    "    #x2 = x[2:6:96,:] ./ 2^14\n",
    "    x = x1 #vcat(x1,x2)\n",
    "    #x = log(log(x+10))\n",
    "    y = y .>  15\n",
    "    #y = (y .==  1) + (y .== 2 )\n",
    "    #Normalize X-Vector\n",
    "    #x = vcat(x ./ sum(x,1),log(findmax(x,1)[1])) #.- 0.5;\n",
    "    return (Array{Float64,2}(full(x)),Array{Float64,1}(full(y)[:,1]))\n",
    "end\n",
    "# SVM k-NN XGBoost Tree-based\n",
    "\n",
    "function GatherTestCase()\n",
    "    \n",
    "    vcatTuple = (i,j) -> (hcat(i[1],j[1]),vcat(i[2],j[2]))\n",
    "    trainingFileSet = map(i -> \"../../../../16-07-\" * dec(i,2) * \".jld\" , collect(2:3))\n",
    "    (x_train, y_train) = reduce(vcatTuple, map(i -> setupTrainingVectorBinary(SparseMatrixCSC{UInt64,Int64}(load(i,\"x\")),load(i,\"y\")),trainingFileSet))\n",
    "    testingFile = \"../../../../16-07-01.jld\"\n",
    "    (x_test,y_test) = setupVectorBinary(SparseMatrixCSC{UInt64,Int64}(load(testingFile,\"x\")),load(testingFile,\"y\"))\n",
    "\n",
    "    testingSetPos = vcat(find(y-> y>0,y_test))\n",
    "    testingSetNeg = vcat(find(y-> y==0,y_test))[1:10000]\n",
    "    println(\"TestingTest dimension\", length(testingSetPos),length(testingSetNeg)) \n",
    "    return (x_train,y_train,x_test,y_test,testingSetPos,testingSetNeg)\n",
    "end\n",
    "function MLLearnNeuralNet()\n",
    "    (x_train,y_train,x_test,y_test,testingSetPos,testingSetNeg) = GatherTestCase()\n",
    "    data  =  MemoryDataLayer(name=\"train-data\",tops=[:data,:label],data=Array[x_train,y_train],batch_size=50)\n",
    "    #conv  = ConvolutionLayer(name=\"conv1\",n_filter=20,kernel=(5,5),bottoms=[:data],tops=[:conv])\n",
    "    #pool  = PoolingLayer(name=\"pool1\",kernel=(2,2),stride=(2,2),bottoms=[:conv],tops=[:pool])\n",
    "    #conv2 = ConvolutionLayer(name=\"conv2\",n_filter=50,kernel=(5,5),bottoms=[:pool],tops=[:conv2])\n",
    "    #pool2 = PoolingLayer(name=\"pool2\",kernel=(2,2),stride=(2,2),bottoms=[:conv2],tops=[:pool2])\n",
    "    fc1   = InnerProductLayer(name=\"ip1\",output_dim=5000,neuron=Neurons.LReLU(),bottoms=[:data], tops=[:ip1])#,weight_init=GaussianInitializer(std=10))\n",
    "    #dr1   = DropoutLayer(name=\"dr1\", ratio=0.5, bottoms=[:ip1]) #,\n",
    "    fc2   = InnerProductLayer(name=\"ip2\",output_dim=2000,neuron=Neurons.LReLU(),bottoms=[:ip1], tops=[:ip2])\n",
    "    fc3   = InnerProductLayer(name=\"ip3\",output_dim=500,neuron=Neurons.LReLU(),bottoms=[:ip2],tops=[:ip3])\n",
    "    fc4   = InnerProductLayer(name=\"ip4\",output_dim=100,neuron=Neurons.LReLU(),bottoms=[:ip3],tops=[:ip4])\n",
    "    fc5   = InnerProductLayer(name=\"ip5\",output_dim=30,neuron=Neurons.LReLU(),bottoms=[:ip4],tops=[:ip5])\n",
    "    fc6   = InnerProductLayer(name=\"ip6\",output_dim=2,neuron=Neurons.LReLU(),bottoms=[:ip5],tops=[:final])\n",
    "    \n",
    "    loss  = SoftmaxLossLayer(name=\"loss\",bottoms=[:final,:label])\n",
    "    backend = DefaultBackend()\n",
    "    init(backend)\n",
    "    common_layers = [fc1,fc2,fc3,fc4,fc5,fc6]\n",
    "    net = Net(\"TweetDetect-train\", backend, [data, common_layers...,loss])\n",
    "    \n",
    "    exp_dir = \"snapshots\"\n",
    "    solver_method = SGD()\n",
    "    params = make_solver_parameters(solver_method, max_iter=6000, regu_coef=0.0005,\n",
    "    mom_policy=MomPolicy.Fixed(0.8),\n",
    "   # lr_policy=LRPolicy.Fixed(0.1))\n",
    "    #lr_policy=LRPolicy.Step(0.007, 0.1,250))\n",
    "    lr_policy=LRPolicy.Inv(0.01, 0.0001, 0.75))\n",
    "    solver = Solver(solver_method, params)\n",
    "    \n",
    "    #setup_coffee_lounge(solver, save_into=\"$exp_dir/statistics.jld\", every_n_iter=1000)\n",
    "    \n",
    "    #report training progress every 100 iterations\n",
    "    add_coffee_break(solver, TrainingSummary(), every_n_iter=500)\n",
    "    \n",
    "    # save snapshots every 5000 iterations\n",
    "    #add_coffee_break(solver, Snapshot(exp_dir), every_n_iter=5000)\n",
    "    \n",
    "    # show performance on test data every 1000 iterations\n",
    "    #data_test = HDF5DataLayer(name=\"test-data\",source=\"test-data-list.txt\",batch_size=100)\n",
    "    \n",
    "    data_test =  MemoryDataLayer(name=\"train-data\",data=Array[x_train, y_train],batch_size=50)\n",
    "    accuracy = AccuracyLayer(name=\"train-accuracy\",bottoms=[:final, :label])\n",
    "    #netplot = BinaryNetPlotLayer(name=\"train-net\",bottoms=[:final, :label])\n",
    "    #rocplot = ROCPlotLayer(name=\"train-net\",bottoms=[:final, :label])\n",
    "    test_train_net = Net(\"TweetDetect-train\", backend, [data_test, common_layers..., accuracy])\n",
    "    add_coffee_break(solver, ValidationPerformance(test_train_net), every_n_iter=1000)\n",
    "    \n",
    "    data_test =  MemoryDataLayer(name=\"test-data\",data=Array[x_test, y_test],batch_size=50)\n",
    "    accuracy = ConfusionMatrixLayer(name=\"test-accuracy\",bottoms=[:final, :label])\n",
    "    outputLayer = MemoryOutputLayer(name=\"test-Output\",bottoms=[:final])\n",
    "    netplot = BinaryNetPlotLayer(name=\"test-net\",bottoms=[:final, :label])\n",
    "    #rocplot = ROCPlotLayer(name=\"test-net\",bottoms=[:final, :label])\n",
    "    test_net = Net(\"TweetDetect-test\", backend, [data_test, common_layers..., accuracy,netplot,outputLayer])\n",
    "    add_coffee_break(solver, ValidationPerformance(test_net), every_n_iter=6000)\n",
    "    \n",
    "    data_test =  MemoryDataLayer(name=\"testPos-data\",data=Array[x_test[:,testingSetPos], y_test[testingSetPos]],batch_size=50)\n",
    "    confMatrix = ConfusionMatrixLayer(name=\"testPos-accuracy\",bottoms=[:final, :label])\n",
    "    accuracy = AccuracyLayer(name=\"testPos-accuracy\",bottoms=[:final, :label])\n",
    "#    netplot = BinaryNetPlotLayer(name=\"testPos-BinaryNetPlot\",bottoms=[:final, :label])\n",
    "#    outputLayer = MemoryOutputLayer(name=\"testPos-Output\",bottoms=[:final])\n",
    "    test_pos_net = Net(\"TweetDetect-testPos\", backend, [data_test, common_layers..., accuracy,confMatrix]) \n",
    "    add_coffee_break(solver, ValidationPerformance(test_pos_net), every_n_iter=2000)\n",
    "    \n",
    "    data_test =  MemoryDataLayer(name=\"testNeg-data\",data=Array[x_test[:,testingSetNeg], y_test[testingSetNeg]],batch_size=50)\n",
    "    accuracy = AccuracyLayer(name=\"testNeg-accuracy\",bottoms=[:final, :label])\n",
    "    test_neg_net = Net(\"TweetDetect-testNeg\", backend, [data_test, common_layers..., accuracy])\n",
    "    add_coffee_break(solver, ValidationPerformance(test_neg_net), every_n_iter=2000)\n",
    "    \n",
    "    #Grabbing the output and analyze based on hashtag.\n",
    "\n",
    "    solve(solver, net) \n",
    "    net_output = solver.coffee_lounge.coffee_breaks[3].coffee.validation_net.states[end].outputs\n",
    "\n",
    "    destroy(net)\n",
    "    destroy(test_train_net)\n",
    "    destroy(test_net)\n",
    "    destroy(test_pos_net)\n",
    "    destroy(test_neg_net)\n",
    "    shutdown(backend)\n",
    "    \n",
    "    test_size = length(y_test)\n",
    "    startidx = test_size+50 - (test_size %50)+ 1\n",
    "    net_output = reduce(hcat,net_output_in[end])[:,startidx:end]\n",
    "    net_output = (findmax(net_output,1)[2] % 3).== 2\n",
    "    net_output = vcat(net_output',zeros(8))\n",
    "    \n",
    "    return (net_output, y_test)\n",
    "end\n",
    "\n",
    "(net_output_in, y_test) = MLLearnNeuralNet()\n",
    "TestTest(net_output_in, y_test,\"NeuralNet\");\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"/home/ac/Desktop/TwitterHashtagPrediction/Julia/Playbooks\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16,2746)\n",
      "(16,2900)\n",
      "TestingTest dimension74610000\n",
      " Iter        f.value       f.change         g.norm           step\n",
      "==================================================================\n",
      "    0     3.9135e+03\n",
      "    1     3.8684e+03    -4.5068e+01     4.1553e+02     3.0518e-05\n",
      "    2     3.6215e+03    -2.4698e+02     6.4474e+02     3.9063e-03\n",
      "    3     3.5047e+03    -1.1676e+02     3.6235e+02     4.8828e-04\n",
      "    4     3.4139e+03    -9.0840e+01     3.3033e+02     3.9063e-03\n",
      "    5     3.3743e+03    -3.9548e+01     3.0594e+02     3.9063e-03\n",
      "    6     3.3615e+03    -1.2774e+01     2.8926e+02     1.9531e-03\n",
      "    7     3.3412e+03    -2.0325e+01     2.7774e+02     3.9063e-03\n",
      "    8     3.3247e+03    -1.6491e+01     2.6854e+02     7.8125e-03\n",
      "    9     3.3113e+03    -1.3425e+01     2.5529e+02     1.5625e-02\n",
      "   10     3.3022e+03    -9.0625e+00     2.4412e+02     1.5625e-02\n",
      "   11     3.2923e+03    -9.9185e+00     2.2404e+02     3.1250e-02\n",
      "   12     3.2793e+03    -1.2992e+01     1.7887e+02     6.2500e-02\n",
      "   13     3.2648e+03    -1.4529e+01     1.2073e+02     6.2500e-02\n",
      "   14     3.2462e+03    -1.8553e+01     5.0878e+01     1.2500e-01\n",
      "   15     3.2333e+03    -1.2922e+01     9.5375e+01     1.2500e-01\n",
      "   16     3.2260e+03    -7.2704e+00     1.2025e+02     2.5000e-01\n",
      "   17     3.2228e+03    -3.2806e+00     7.9105e+01     5.0000e-01\n",
      "   18     3.2211e+03    -1.6274e+00     4.1425e+00     1.0000e+00\n",
      "   19     3.2211e+03    -7.4801e-02     2.5113e+00     1.0000e+00\n",
      "   20     3.2210e+03    -1.3524e-02     1.9353e+00     1.0000e+00\n",
      "   21     3.2210e+03    -1.3179e-03     3.2941e-01     1.0000e+00\n",
      "   22     3.2210e+03    -2.1954e-05     2.7600e-02     1.0000e+00\n",
      "Converged with 22 iterations @ f.value = 3.2210e+03\n",
      "381171\n",
      "746.0\n",
      "33\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PyPlot.Figure(PyObject <matplotlib.figure.Figure object at 0x7f9e33f0c4d0>)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7687\n",
      "True Positives Count:33\n",
      "True Negative Count:116069\n",
      "False Positives Count:7654\n",
      "False Negative Count:0\n",
      "Statistics VS TnT\n",
      "TruePositiveRate: 1.0 vs TnT 95%\n",
      "FalsePositiveRate: 0.061864002651083466 vs TnT 4%\n"
     ]
    }
   ],
   "source": [
    "#Regression Based.\n",
    "using Regression\n",
    "function MLLearnRegression()\n",
    "    (x_train,y_train,x_test,y_test,testingSetPos,testingSetNeg) = GatherTestCase()\n",
    "    #data  =  MemoryDataLayer(name=\"train-data\",tops=[:data,:label],data=Array[x_train,y_train],batch_size=50)\n",
    "    \n",
    "    n = length(x_train)   # number of samples\n",
    "\n",
    "    # perform estimation\n",
    "    ret = Regression.solve(\n",
    "        logisticreg(x_train, sign(y_train-0.5); bias=1.0),   # construct a logistic regression problem\n",
    "        reg=SqrL2Reg(1.0e-2),          # apply squared L2 regularization\n",
    "    options=Regression.Options(verbosity=:iter, grtol=1.0e-6 * n))  # set options\n",
    "    \n",
    "    # extract results\n",
    "    #w_e = ret.sol\n",
    "    #net_output = w_e*x_test\n",
    "    #net_output = solver.coffee_lounge.coffee_breaks[3].coffee.validation_net.states[end].outputs\n",
    "    w_e = ret.sol\n",
    "    net_output = (w_e[1:end-1]'*x_test .+ w_e[end]).>0\n",
    "    return (net_output, y_test)\n",
    "end\n",
    "(net_output_in, y_test) = MLLearnRegression()\n",
    "TestTest(net_output_in, y_test,\"Regression\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLLearnXGBoost (generic function with 1 method)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Using XGBoost\n",
    "using XGBoost\n",
    "function MLLearnXGBoost()\n",
    "    (x_train,y_train,x_test,y_test,testingSetPos,testingSetNeg) = GatherTestCase()\n",
    "\n",
    "    num_round = 20\n",
    "    #return x_train,y_train\n",
    "    bst = xgboost(x_train', num_round, label=y_train, eta=0.9, max_depth=10)\n",
    "    pred = XGBoost.predict(bst, x_test')\n",
    "    #return pred\n",
    "    println(\"test-error=\", sum(Array{Int,1}(pred .> 0.5) .!= (y_test.>0.5)) / float(size(pred)[1]))\n",
    "    \n",
    "    return (sparse(Array{Int,1}(pred.>0.5)),y_test)\n",
    "end\n",
    "(net_output_in, y_test) =  MLLearnXGBoost()\n",
    "TestTest(net_output_in, y_test,\"XGBoost\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16,2746)\n",
      "(16,2900)\n",
      "TestingTest dimension74610000\n",
      "146406\n",
      "746.0\n",
      "33\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PyPlot.Figure(PyObject <matplotlib.figure.Figure object at 0x7f9e33b90290>)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2190\n",
      "True Positives Count:33\n",
      "True Negative Count:121566\n",
      "False Positives Count:2157\n",
      "False Negative Count:0\n",
      "Statistics VS TnT\n",
      "TruePositiveRate: 1.0 vs TnT 95%\n",
      "FalsePositiveRate: 0.01743410683543076 vs TnT 4%\n"
     ]
    }
   ],
   "source": [
    "#kNN Approach\n",
    "using NearestNeighbors\n",
    "function MLLearnNN()\n",
    "    \n",
    "    (x_train,y_train,x_test,y_test,testingSetPos,testingSetNeg) = GatherTestCase()\n",
    "\n",
    "    \n",
    "    kdtree = KDTree(x_train)\n",
    "    k = 5\n",
    "    \n",
    "    net_output = [(k*0.8).<sum(y_train[knn(kdtree,x_test[:,i],k,true)[1]]) for i in 1:length(y_test)]\n",
    "    return (sparse(net_output),y_test)\n",
    "    \n",
    "end\n",
    "(net_output_in,y_test) = MLLearnNN();\n",
    "TestTest(net_output_in, y_test,\"k-NN\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "160435\n",
      "746.0\n",
      "33\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PyPlot.Figure(PyObject <matplotlib.figure.Figure object at 0x7f3396262b10>)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2454\n",
      "True Positives Count:32\n",
      "True Negative Count:121301\n",
      "False Positives Count:2422\n",
      "False Negative Count:1\n",
      "Statistics VS TnT\n",
      "TruePositiveRate: 0.9696969696969697 vs TnT 95%\n",
      "FalsePositiveRate: 0.019575988296436395 vs TnT 4%\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "LoadError: MethodError: `.<` has no method matching .<(::Float64, ::Void)\nClosest candidates are:\n  .<(::Real, !Matched::Real)\n  .<(::Any, !Matched::AbstractArray{T,N})\n  .<(!Matched::AbstractArray{T,N}, ::Any)\nwhile loading In[74], in expression starting on line 1",
     "output_type": "error",
     "traceback": [
      "LoadError: MethodError: `.<` has no method matching .<(::Float64, ::Void)\nClosest candidates are:\n  .<(::Real, !Matched::Real)\n  .<(::Any, !Matched::AbstractArray{T,N})\n  .<(!Matched::AbstractArray{T,N}, ::Any)\nwhile loading In[74], in expression starting on line 1",
      "",
      " in .> at operators.jl:39"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.4.0",
   "language": "julia",
   "name": "julia-0.4"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
