{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "recurrentFixedWindow (generic function with 1 method)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using JSON\n",
    "import GZip\n",
    "#Spark-Like Functions\n",
    "function mapByKey(f,d)\n",
    "    nd = Dict{keytype(d),Any}()\n",
    "    for k in keys(d)\n",
    "        nd[k] = f(d[k])\n",
    "    end\n",
    "    return nd\n",
    "end\n",
    "\n",
    "\n",
    "function reduceByKeyAndWindow(f,win,slid,dstr)\n",
    "    #Reduce by window and Key\n",
    "    #Takes in a collection of values of a key and a certain window size (win:Size of window in array slots)\n",
    "    #with slide factor (slid), and reduce by a function, (f:binary reduce function).\n",
    "    #takes in key-value dstream (dstr: Array{Dict,1})\n",
    "    \n",
    "    #initialize new dstr\n",
    "    newDstrSize = round(Int64,ceil(length(dstr)/slid))\n",
    "    newDstr = Array{Dict,1}(newDstrSize)\n",
    "    #for loop with sliding factor\n",
    "    for i in collect(1:newDstrSize)\n",
    "        #for each sliding loop creates an array of keys,\n",
    "        currI = (i-1)*slid+1\n",
    "        currKeys = reduce(vcat,map(x -> collect(keys(x)), dstr[max(currI-win,1):(currI)]))\n",
    "        #subsequently populate an dict for each keys\n",
    "        currDict = Dict{keytype(dstr[1]),valtype(dstr[1])}()\n",
    "        for k in currKeys\n",
    "            #collect all value in the window\n",
    "            currKeyCol = Array{valtype(dstr[1]),1}()\n",
    "            for j in collect(max(currI-win,1):currI)\n",
    "                if haskey(dstr[j],k)\n",
    "                    push!(currKeyCol,dstr[j][k])\n",
    "                end\n",
    "            end\n",
    "            #Then reduce.\n",
    "            currDict[k] = reduce(f,currKeyCol)\n",
    "        end\n",
    "        newDstr[i] = currDict\n",
    "    end\n",
    "    return newDstr\n",
    "end\n",
    "\n",
    "function mapReduceByKey(mapF,reduceF, dict)\n",
    "    return Dict(map(x -> Pair(x[1],mapreduce(mapF,reduceF,x[2])),dict))\n",
    "end\n",
    "\n",
    "function recurrentFixedWindow(t,dstr)\n",
    "    #construct numeric recurrent vector from a fixed length sliding window\n",
    "    #with default values of zero\n",
    "    #assume all vector within any single slot is of same length.\n",
    "    #input is an array of Dicts of string to (numeric array or number)\n",
    "    \n",
    "    #First let's determine n the number of array of a single slot.\n",
    "    n = length(first(dstr[1])[2])\n",
    "    #let's determine l, the number of windows the operation will result in.\n",
    "    l = length(dstr) - t\n",
    "    newdst = Array{Dict{keytype(dstr[1]),Array{Number,1}},1}(l)\n",
    "    for i in collect(1:l)\n",
    "        #TODO make sliding window operation to optimize performance\n",
    "        #create an array of possible keys\n",
    "        currKeys = reduce(vcat,map(x -> collect(keys(x)), dstr[i:(i+t)]))\n",
    "        currDict = Dict{keytype(dstr[1]),Array{Number,1}}()\n",
    "        #for each of the keys we'll populate a recurrent vector.\n",
    "        for k in currKeys\n",
    "            currArr = Array{Number,1}()\n",
    "            for j in collect(i:(i+t))\n",
    "                if haskey(dstr[j],k)\n",
    "                    currArr = vcat(currArr,dstr[j][k])\n",
    "                else\n",
    "                    #Note that default value is filled if the key doesn't exist within a slot.\n",
    "                    append!(currArr,zeros(n))\n",
    "                end\n",
    "            end\n",
    "            currDict[k]=currArr\n",
    "        end\n",
    "        newdst[i] = currDict\n",
    "    end\n",
    "    return newdst\n",
    "end\n",
    "\n",
    "\n",
    "#File Read In\n",
    "#x = [JSON.parse(readlines(GZip.open(\"../16-06-17/\"*x))) for x in readdir(\"../16-06-17\")]; #File Readin\n",
    "#x = mapreduce(x->map(y -> JSON.parse(y),readlines(GZip.open(\"../16-06-17/\"*x))),vcat,readdir(\"../16-06-17\")[1:6])\n",
    "#This is the old file read in.  works well for 1 file.  Thus we will have to load&parse each file independently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "toDstream (generic function with 1 method)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filter out Tweets from a mix of delete and tweets\n",
    "#split into windows\n",
    "function toDstream(inX,t) #t=120000\n",
    "    println(\"Begin binning to 2 minutes slots\")\n",
    "    x= inX[findn([\"created_at\" in keys(y) for y in inX])];\n",
    "    dt = join(split(x[1][\"created_at\"],\" \")[[1,2,3,4,6]],\" \")\n",
    "    startTime = DateTime(dt,\"e u d H:M:S y\")\n",
    "    dt = join(split(x[end][\"created_at\"],\" \")[[1,2,3,4,6]],\" \")\n",
    "    endTime = DateTime(dt,\"e u d H:M:S y\")\n",
    "    println(startTime)\n",
    "    windowCount = 1+round(Int,ceil(Dates.value(endTime - startTime) /t ))#Divid into 2 minutes windows\n",
    "    Tweets = Array{Array{Dict}}(windowCount)\n",
    "    for i in collect(1:windowCount)\n",
    "        Tweets[i] = Array{Dict,1}()\n",
    "    end\n",
    "    print(\"windowCount:\")\n",
    "    println(windowCount)\n",
    "    for tweet in x\n",
    "        dt = join(split(tweet[\"created_at\"],\" \")[[1,2,3,4,6]],\" \")\n",
    "        currTime = DateTime(dt,\"e u d H:M:S y\")\n",
    "        windowIndex = round(Int,ceil(Dates.value(currTime - startTime) /t ))#Divid into 2 minutes windows \n",
    "        if (windowIndex >= 0) && (windowIndex < (windowCount))\n",
    "            append!(Tweets[windowIndex+1],[tweet])\n",
    "        else\n",
    "            println(windowIndex)\n",
    "        end\n",
    "    end\n",
    "    #Tweets[1][1] #First 10 sec Window's First tweet.\n",
    "    return Tweets\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "toHashMap (generic function with 1 method)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Compile all tweets Per finds hashs within a single tweet\n",
    "function findHash(x)\n",
    "    words = split(x,\" \")\n",
    "    return words[find(word -> (length(word) > 1)&& (word[1] == '#'),words)]\n",
    "end\n",
    "#findHash(x[20][\"text\"])\n",
    "#x[20][\"text\"]\n",
    "function extractHashMap(x) #calculates per window\n",
    "    z = mapreduce(i-> [(y,i) for y in findHash(i[\"text\"])],vcat,x);\n",
    "    # z is the table of hash to a list of tweets\n",
    "    c = Dict{AbstractString,Array}();\n",
    "    for i in z\n",
    "        if !haskey(c,i[1])\n",
    "            c[i[1]] = []\n",
    "        end\n",
    "        append!(c[i[1]],[i[2]])\n",
    "    end\n",
    "    return c\n",
    "end\n",
    "function toHashMap(Tweetdstr)\n",
    "    println(\"Begin sorting into Hashes\")\n",
    "    HashTweetMap = map(extractHashMap,Tweetdstr)#Dict{Hash, Tweets} per 10 secs window\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "toXVectors (generic function with 1 method)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Compile ML input Vectors\n",
    "function toVectorDstr(HashTweetMap)\n",
    "    println(\"Conversion to Vectors\")\n",
    "    vectorSize = 1\n",
    "    vectorIndMapping = Array{Tuple{Int64,AbstractString},1}()\n",
    "    x_vector = Array{Array{Number,1},1}()\n",
    "    \n",
    "    ##Volume\n",
    "    function calculateVolume(x) #calculates per window for all hash [tweets] key pairs\n",
    "        map(v -> length(v[2]),x)\n",
    "    end\n",
    "    HashVolumeMap = map(x-> mapByKey(length,x),HashTweetMap)\n",
    "    #HashVolumeMap = reduceByKeyAndWindow((+),10,1,HashVolumeMap) #This is sum into overlapping 100 secs windows\n",
    "    \n",
    "    ##Unique Users Count\n",
    "    HashUserMap = map(z -> Dict(map(x-> Pair(x[1],unique(map(y-> y[\"user\"][\"name\"],x[2]))),z)),HashTweetMap)\n",
    "    #HashUserMap = reduceByKeyAndWindow(vcat,10,1,HashUserMap) #Organize into 100 secs windows\n",
    "    HashUserMap = map(z -> Dict(map(x-> Pair(x[1],length(unique(x[2]))),z)),HashUserMap)\n",
    "    \n",
    "    \n",
    "    ## 1st degree Connected Tweets\n",
    "    HashDegreeMap = map(z -> Dict(map(x-> Pair(x[1],unique(map(y-> findHash(y[\"text\"]),x[2]))),z)),HashTweetMap)\n",
    "    #HashDegreeMap = reduceByKeyAndWindow(vcat,10,1,HashDegreeMap) #Organize into 100 secs windows\n",
    "    HashDegreeMap = map(z -> Dict(map(x-> Pair(x[1],unique(x[2])),z)),HashDegreeMap)\n",
    "    HashDegreeMap = map(z -> Dict(map(x-> Pair(x[1],mapreduce(y-> HashVolumeMap[z][y[1]],(+),vcat(x[2]))),HashDegreeMap[z])),1:length(HashDegreeMap))\n",
    "    \n",
    "    #Summarize into x_vector essentially flatmap into keyless\n",
    "    HashVectorMap = Array{Dict{AbstractString,Any},1}(length(HashVolumeMap))\n",
    "    \n",
    "    for i in collect(1:length(HashVolumeMap))\n",
    "        currDict = Dict{AbstractString,Any}()\n",
    "        for k in keys(HashVolumeMap[i])\n",
    "            currDict[k] = [HashUserMap[i][k],HashVolumeMap[i][k],HashDegreeMap[i][k]]\n",
    "        end\n",
    "        HashVectorMap[i] = currDict\n",
    "    end\n",
    "    \n",
    "    return HashVectorMap\n",
    "end\n",
    "\n",
    "function toXVectors(VectorDStr)\n",
    "    recurrentCount = 5\n",
    "    #vectorIndMapping = Array{Tuple{Int64,AbstractString},1}()\n",
    "    x_vector = Array{Array{Number,1},1}()\n",
    "    HashVectorMap = recurrentFixedWindow(recurrentCount,VectorDStr)\n",
    "    newdict = Array{Dict{keytype(VectorDStr[1]),Integer},1}(length(HashVectorMap))\n",
    "    for i in collect(1:length(HashVectorMap))\n",
    "        newdict[i] = Dict{keytype(VectorDStr[1]),Integer}() \n",
    "    end\n",
    "    for i in collect(1:length(HashVectorMap))\n",
    "        for k in keys(HashVectorMap[i])\n",
    "            push!(x_vector, HashVectorMap[i][k])\n",
    "            newdict[i][k] = length(x_vector)\n",
    "        end\n",
    "    end\n",
    "    return (x_vector, newdict)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin binning to 2 minutes slots\n",
      "2016-06-17T03:49:59\n",
      "windowCount:6\n",
      "Begin sorting into Hashes\n",
      "Conversion to Vectors\n",
      "Begin binning to 2 minutes slots\n",
      "2016-06-17T03:59:46\n",
      "windowCount:7\n",
      "Begin sorting into Hashes\n",
      "Conversion to Vectors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Actually running each file one by one.  #First two hours\n",
    "x_store = mapreduce(x->toVectorDstr(toHashMap(toDstream(map(y -> JSON.parse(y),readlines(GZip.open(\"/home/ac/TwitterData/16-06-17/\"*x))),120000))),vcat,readdir(\"/home/ac/TwitterData/16-06-17\")[1:2])\n",
    "1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#getting training Truths\n",
    "(x_vector,vectorIndMapping) = toXVectors(x_store)\n",
    "using JSON\n",
    "import GZip\n",
    "trendx = mapreduce(x->map(y -> JSON.parse(y),readlines(GZip.open(\"/home/ac/TwitterData/trend16-06-17/\"*x))),vcat,readdir(\"/home/ac/TwitterData/trend16-06-17\")[1:6]) #File Readin\n",
    "trendx = mapreduce(i -> mapreduce(j-> Pair(i[\"timestamp_ms\"],j[\"Name\"]),vcat,i[\"Trend\"]),vcat,trendx)\n",
    "trendingHashes = unique(trendx[find(i -> i[2][1] == '#',trendx)])\n",
    "recurrentCount = 5\n",
    "#dt = Dates.datetime2unix(startTime)*1000\n",
    "dt = first(trendingHashes)[1]\n",
    "trendingHashes = map(x -> Pair(round(Int64, (x[1] - dt)/120000),x[2]), trendingHashes)\n",
    "y_vector = -ones(length(x_vector))\n",
    "for x in trendingHashes\n",
    "    recurrentIndex = x[1] - recurrentCount\n",
    "    if (recurrentIndex > 0)\n",
    "        for i in collect(0:10)#Window to declare subsignal a trend from declaration.\n",
    "            if (x[1]-i > 0) && (x[1]-i < length(vectorIndMapping)) && haskey(vectorIndMapping[x[1]-i],x[2])\n",
    "                y_vector[vectorIndMapping[x[1]-i][x[2]]] = 1\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "end\n",
    "1\n",
    "#print(find(x-> x== (2,\"#reddit\"), vectorIndMapping))\n",
    "#sum([!haskey(c,i) for i in trendingHashes]) #TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempt#1: Logistical Regression\n"
     ]
    },
    {
     "ename": "LoadError",
     "evalue": "LoadError: MethodError: `size` has no method matching size(::Pair{Int64,ASCIIString})\nClosest candidates are:\n  size(::Any, !Matched::Integer, !Matched::Integer, !Matched::Integer...)\nwhile loading In[92], in expression starting on line 8",
     "output_type": "error",
     "traceback": [
      "LoadError: MethodError: `size` has no method matching size(::Pair{Int64,ASCIIString})\nClosest candidates are:\n  size(::Any, !Matched::Integer, !Matched::Integer, !Matched::Integer...)\nwhile loading In[92], in expression starting on line 8",
      ""
     ]
    }
   ],
   "source": [
    "##### Initilzie ML\n",
    "\n",
    "###First Logistical Regression Methods\n",
    "using Regression\n",
    "println(\"Attempt#1: Logistical Regression\")\n",
    "X = Array{Float64,2}(reduce(hcat,x_vector));\n",
    "y = sign(y_vector);\n",
    "(d,n) = size(X)\n",
    "#Uniform training data\n",
    "#X_train = X[:,1:round(Int64,n/2)]\n",
    "#y_train = y[1:round(Int64,n/2)]\n",
    "#Sampled Biased training data.\n",
    "    trainN = length(find(x -> x == 1, y_train))\n",
    "    trainSample = vcat(find(x -> x == -1, y_train)[1:trainN],find(x -> x == 1, y_train))\n",
    "    y_train = y_train[trainSample]\n",
    "    X_train = X_train[:,trainSample]\n",
    "# perform estimation #Taken from Regression.jl FrontPage\n",
    "ret = Regression.solve(\n",
    "logisticreg(X_train, y_train; bias=1.0),   # construct a logistic regression problem\n",
    "    #solver=GD(),\n",
    "    reg=SqrL2Reg(1.0e-2),          # apply squared L2 regularization\n",
    "options=Regression.Options(maxiter =200,verbosity=:final, grtol=1.0e-6 * n))  # set options\n",
    "#Test Solution\n",
    "X_test = X[:,round(Int64,n/2):end]\n",
    "y_test = y[round(Int64,n/2):end]\n",
    "w_hat = ret.sol\n",
    "y_hat = sign(X_test'w_hat[1:d] +w_hat[d+1] )\n",
    "errorVec = y_test-y_hat\n",
    "errorRate = sum(abs(errorVec))*0.5 / (length(errorVec)) \n",
    "print(\"Logistical Regression Grand Error Rate Against Test = \")\n",
    "println(errorRate)\n",
    "errorVec = errorVec[find(x -> x == 1, y_test)]\n",
    "errorRate = sum(abs(errorVec))*0.5 / (length(errorVec)) \n",
    "print(\"Logistical Regression False Positive Rate Against Test = \")\n",
    "println(errorRate)\n",
    "\n",
    "###Neural Net  Mocha.jl\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19-Jul 14:42:07:INFO:root:Constructing net TweetDetect-train on Mocha.CPUBackend...\n",
      "19-Jul 14:42:07:INFO:root:Topological sorting 5 layers...\n",
      "19-Jul 14:42:07:INFO:root:Setup layers...\n",
      "19-Jul 14:42:07:INFO:root:Network constructed!\n",
      "19-Jul 14:42:07:INFO:root:Constructing net TweetDetect-test on Mocha.CPUBackend...\n",
      "19-Jul 14:42:07:INFO:root:Topological sorting 5 layers...\n",
      "19-Jul 14:42:07:INFO:root:Setup layers...\n",
      "19-Jul 14:42:07:DEBUG:root:InnerProductLayer(ip1): sharing weights and bias\n",
      "19-Jul 14:42:07:DEBUG:root:InnerProductLayer(ip3): sharing weights and bias\n",
      "19-Jul 14:42:07:DEBUG:root:InnerProductLayer(ip2): sharing weights and bias\n",
      "19-Jul 14:42:07:INFO:root:Network constructed!\n",
      "19-Jul 14:42:07:INFO:root:Constructing net TweetDetect-testPos on Mocha.CPUBackend...\n",
      "19-Jul 14:42:07:INFO:root:Topological sorting 5 layers...\n",
      "19-Jul 14:42:07:INFO:root:Setup layers...\n",
      "19-Jul 14:42:07:DEBUG:root:InnerProductLayer(ip1): sharing weights and bias\n",
      "19-Jul 14:42:07:DEBUG:root:InnerProductLayer(ip3): sharing weights and bias\n",
      "19-Jul 14:42:07:DEBUG:root:InnerProductLayer(ip2): sharing weights and bias\n",
      "19-Jul 14:42:07:INFO:root:Network constructed!\n",
      "19-Jul 14:42:08:INFO:root:Constructing net TweetDetect-testNeg on Mocha.CPUBackend...\n",
      "19-Jul 14:42:08:INFO:root:Topological sorting 5 layers...\n",
      "19-Jul 14:42:08:INFO:root:Setup layers...\n",
      "19-Jul 14:42:08:DEBUG:root:InnerProductLayer(ip1): sharing weights and bias\n",
      "19-Jul 14:42:08:DEBUG:root:InnerProductLayer(ip3): sharing weights and bias\n",
      "19-Jul 14:42:08:DEBUG:root:InnerProductLayer(ip2): sharing weights and bias\n",
      "19-Jul 14:42:08:INFO:root:Network constructed!\n",
      "19-Jul 14:42:09:DEBUG:root:#DEBUG Checking network topology for back-propagation\n",
      "19-Jul 14:42:10:DEBUG:root:Init network TweetDetect-train\n",
      "19-Jul 14:42:10:DEBUG:root:Init parameter weight for layer ip1\n",
      "19-Jul 14:42:10:DEBUG:root:Init parameter bias for layer ip1\n",
      "19-Jul 14:42:10:DEBUG:root:Init parameter weight for layer ip3\n",
      "19-Jul 14:42:10:DEBUG:root:Init parameter bias for layer ip3\n",
      "19-Jul 14:42:10:DEBUG:root:Init parameter weight for layer ip2\n",
      "19-Jul 14:42:10:DEBUG:root:Init parameter bias for layer ip2\n",
      "19-Jul 14:42:11:DEBUG:root:#DEBUG Initializing coffee breaks\n",
      "19-Jul 14:42:11:DEBUG:root:Init network TweetDetect-test\n",
      "19-Jul 14:42:11:DEBUG:root:Init network TweetDetect-testPos\n",
      "19-Jul 14:42:11:DEBUG:root:Init network TweetDetect-testNeg\n",
      "19-Jul 14:42:11:INFO:root: TRAIN iter=000000 obj_val=0.93918921\n",
      "19-Jul 14:42:11:INFO:root:\n",
      "19-Jul 14:42:11:INFO:root:## Performance on Validation Set after 0 iterations\n",
      "19-Jul 14:42:11:INFO:root:---------------------------------------------------------\n",
      "19-Jul 14:42:11:INFO:root:  Accuracy (avg over 300) = 59.0000%\n",
      "19-Jul 14:42:11:INFO:root:---------------------------------------------------------\n",
      "19-Jul 14:42:11:INFO:root:\n",
      "19-Jul 14:42:11:INFO:root:\n",
      "19-Jul 14:42:11:INFO:root:## Performance on Validation Set after 0 iterations\n",
      "19-Jul 14:42:11:INFO:root:---------------------------------------------------------\n",
      "19-Jul 14:42:11:INFO:root:  Accuracy (avg over 150) = 14.0000%\n",
      "19-Jul 14:42:11:INFO:root:---------------------------------------------------------\n",
      "19-Jul 14:42:11:INFO:root:\n",
      "19-Jul 14:42:13:INFO:root:\n",
      "19-Jul 14:42:13:INFO:root:## Performance on Validation Set after 0 iterations\n",
      "19-Jul 14:42:13:INFO:root:---------------------------------------------------------\n",
      "19-Jul 14:42:13:INFO:root:  Accuracy (avg over 36800) = 83.5326%\n",
      "19-Jul 14:42:13:INFO:root:---------------------------------------------------------\n",
      "19-Jul 14:42:13:INFO:root:\n",
      "19-Jul 14:42:13:DEBUG:root:#DEBUG Entering solver loop\n",
      "19-Jul 14:42:23:INFO:root: TRAIN iter=001000 obj_val=0.25994549\n",
      "19-Jul 14:42:23:INFO:root:\n",
      "19-Jul 14:42:23:INFO:root:## Performance on Validation Set after 1000 iterations\n",
      "19-Jul 14:42:23:INFO:root:---------------------------------------------------------\n",
      "19-Jul 14:42:23:INFO:root:  Accuracy (avg over 300) = 87.0000%\n",
      "19-Jul 14:42:23:INFO:root:---------------------------------------------------------\n",
      "19-Jul 14:42:23:INFO:root:\n",
      "19-Jul 14:42:23:INFO:root:\n",
      "19-Jul 14:42:23:INFO:root:## Performance on Validation Set after 1000 iterations\n",
      "19-Jul 14:42:23:INFO:root:---------------------------------------------------------\n",
      "19-Jul 14:42:23:INFO:root:  Accuracy (avg over 150) = 67.3333%\n",
      "19-Jul 14:42:23:INFO:root:---------------------------------------------------------\n",
      "19-Jul 14:42:23:INFO:root:\n",
      "19-Jul 14:42:25:INFO:root:\n",
      "19-Jul 14:42:25:INFO:root:## Performance on Validation Set after 1000 iterations\n",
      "19-Jul 14:42:25:INFO:root:---------------------------------------------------------\n",
      "19-Jul 14:42:25:INFO:root:  Accuracy (avg over 36800) = 92.0897%\n",
      "19-Jul 14:42:25:INFO:root:---------------------------------------------------------\n",
      "19-Jul 14:42:25:INFO:root:\n",
      "19-Jul 14:42:35:INFO:root: TRAIN iter=002000 obj_val=0.25434309\n",
      "19-Jul 14:42:35:INFO:root:\n",
      "19-Jul 14:42:35:INFO:root:## Performance on Validation Set after 2000 iterations\n",
      "19-Jul 14:42:35:INFO:root:---------------------------------------------------------\n",
      "19-Jul 14:42:35:INFO:root:  Accuracy (avg over 300) = 86.6667%\n",
      "19-Jul 14:42:35:INFO:root:---------------------------------------------------------\n",
      "19-Jul 14:42:35:INFO:root:\n",
      "19-Jul 14:42:35:INFO:root:\n",
      "19-Jul 14:42:35:INFO:root:## Performance on Validation Set after 2000 iterations\n",
      "19-Jul 14:42:35:INFO:root:---------------------------------------------------------\n",
      "19-Jul 14:42:35:INFO:root:  Accuracy (avg over 100) = 60.0000%\n",
      "19-Jul 14:42:35:INFO:root:---------------------------------------------------------\n",
      "19-Jul 14:42:35:INFO:root:\n",
      "19-Jul 14:42:37:INFO:root:\n",
      "19-Jul 14:42:37:INFO:root:## Performance on Validation Set after 2000 iterations\n",
      "19-Jul 14:42:37:INFO:root:---------------------------------------------------------\n",
      "19-Jul 14:42:37:INFO:root:  Accuracy (avg over 36750) = 92.4517%\n",
      "19-Jul 14:42:37:INFO:root:---------------------------------------------------------\n",
      "19-Jul 14:42:37:INFO:root:\n",
      "19-Jul 14:42:49:INFO:root: TRAIN iter=003000 obj_val=0.25111998\n",
      "19-Jul 14:42:49:INFO:root:\n"
     ]
    },
    {
     "ename": "LoadError",
     "evalue": "LoadError: InterruptException:\nwhile loading In[33], in expression starting on line 66",
     "output_type": "error",
     "traceback": [
      "LoadError: InterruptException:\nwhile loading In[33], in expression starting on line 66",
      "",
      " in gemm! at linalg/blas.jl:633",
      " in forward at /home/ac/.julia/v0.4/Mocha/src/layers/inner-product.jl:125",
      " in forward at /home/ac/.julia/v0.4/Mocha/src/net.jl:148",
      " in forward_epoch at /home/ac/.julia/v0.4/Mocha/src/net.jl:140",
      " in enjoy at /home/ac/.julia/v0.4/Mocha/src/coffee/validation-performance.jl:26",
      " in check_coffee_break at /home/ac/.julia/v0.4/Mocha/src/coffee-break.jl:140",
      " in onestep_solve at /home/ac/.julia/v0.4/Mocha/src/solvers.jl:230",
      " in do_solve_loop at /home/ac/.julia/v0.4/Mocha/src/solvers.jl:242",
      " in solve at /home/ac/.julia/v0.4/Mocha/src/solvers.jl:235"
     ]
    }
   ],
   "source": [
    "#ProtoType Shop\n",
    "using Mocha\n",
    "y_vector = (y_vector +1) ./2\n",
    "trainingSet = vcat(find(y-> y==1,y_vector)[1:25],collect(1:75))\n",
    "testingSet = vcat(find(y-> y==1,y_vector)[1:100],collect(1:200))\n",
    "testingSetPos = vcat(find(y-> y==1,y_vector))\n",
    "testingSetNeg = vcat(find(y-> y==0,y_vector))\n",
    "data  =  MemoryDataLayer(name=\"train-data\",tops=[:data,:label],data=Array[mapreduce(i -> Array{Float64}(i),hcat,x_vector[trainingSet]), \n",
    "y_vector[trainingSet]],batch_size=25)\n",
    "#conv  = ConvolutionLayer(name=\"conv1\",n_filter=20,kernel=(5,5),bottoms=[:data],tops=[:conv])\n",
    "#pool  = PoolingLayer(name=\"pool1\",kernel=(2,2),stride=(2,2),bottoms=[:conv],tops=[:pool])\n",
    "#conv2 = ConvolutionLayer(name=\"conv2\",n_filter=50,kernel=(5,5),bottoms=[:pool],tops=[:conv2])\n",
    "#pool2 = PoolingLayer(name=\"pool2\",kernel=(2,2),s  tride=(2,2),bottoms=[:conv2],tops=[:pool2])\n",
    "fc1   = InnerProductLayer(name=\"ip1\",output_dim=500,neuron=Neurons.ReLU(),bottoms=[:data],\n",
    "                          tops=[:ip1])\n",
    "fc3   = InnerProductLayer(name=\"ip3\",output_dim=700,neuron=Neurons.ReLU(),bottoms=[:ip1],\n",
    "                          tops=[:ip3])\n",
    "fc2   = InnerProductLayer(name=\"ip2\",output_dim=2,bottoms=[:ip3],tops=[:ip2])\n",
    "loss  = SoftmaxLossLayer(name=\"loss\",bottoms=[:ip2,:label])\n",
    "\n",
    "backend = DefaultBackend()\n",
    "init(backend)\n",
    "\n",
    "common_layers = [fc1,fc3, fc2]\n",
    "net = Net(\"TweetDetect-train\", backend, [data, common_layers..., loss])\n",
    "\n",
    "exp_dir = \"snapshots\"\n",
    "solver_method = SGD()\n",
    "params = make_solver_parameters(solver_method, max_iter=10000, regu_coef=0.0005,\n",
    "    mom_policy=MomPolicy.Fixed(0.9),\n",
    "    lr_policy=LRPolicy.Inv(0.01, 0.0001, 0.75))\n",
    "solver = Solver(solver_method, params)\n",
    "\n",
    "#setup_coffee_lounge(solver, save_into=\"$exp_dir/statistics.jld\", every_n_iter=1000)\n",
    "\n",
    "#report training progress every 100 iterations\n",
    "add_coffee_break(solver, TrainingSummary(), every_n_iter=1000)\n",
    "\n",
    "# save snapshots every 5000 iterations\n",
    "#add_coffee_break(solver, Snapshot(exp_dir), every_n_iter=5000)\n",
    "\n",
    "# show performance on test data every 1000 iterations\n",
    "#data_test = HDF5DataLayer(name=\"test-data\",source=\"test-data-list.txt\",batch_size=100)\n",
    "data_test =  MemoryDataLayer(name=\"test-data\",data=Array[mapreduce(i -> Array{Float64}(i),hcat,x_vector[testingSet]), \n",
    "    y_vector[testingSet]],batch_size=50)\n",
    "accuracy = AccuracyLayer(name=\"test-accuracy\",bottoms=[:ip2, :label])\n",
    "test_net = Net(\"TweetDetect-test\", backend, [data_test, common_layers..., accuracy])\n",
    "add_coffee_break(solver, ValidationPerformance(test_net), every_n_iter=1000)\n",
    "\n",
    "data_test =  MemoryDataLayer(name=\"testPos-data\",data=Array[mapreduce(i -> Array{Float64}(i),hcat,x_vector[testingSetPos]), \n",
    "    y_vector[testingSetPos]],batch_size=50)\n",
    "accuracy = AccuracyLayer(name=\"testPos-accuracy\",bottoms=[:ip2, :label])\n",
    "test_net = Net(\"TweetDetect-testPos\", backend, [data_test, common_layers..., accuracy])\n",
    "add_coffee_break(solver, ValidationPerformance(test_net), every_n_iter=1000)\n",
    "\n",
    "\n",
    "\n",
    "data_test =  MemoryDataLayer(name=\"testNeg-data\",data=Array[mapreduce(i -> Array{Float64}(i),hcat,x_vector[testingSetNeg]), \n",
    "    y_vector[testingSetNeg]],batch_size=50)\n",
    "accuracy = AccuracyLayer(name=\"testNeg-accuracy\",bottoms=[:ip2, :label])\n",
    "test_net = Net(\"TweetDetect-testNeg\", backend, [data_test, common_layers..., accuracy])\n",
    "add_coffee_break(solver, ValidationPerformance(test_net), every_n_iter=1000)\n",
    "\n",
    "\n",
    "\n",
    "solve(solver, net)\n",
    "\n",
    "destroy(net)\n",
    "destroy(test_net)\n",
    "shutdown(backend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin binning to 2 minutes slots\n",
      "2016-06-17T03:49:59\n",
      "Begin sorting into Hashes\n",
      "Begin binning to 2 minutes slots\n",
      "2016-06-17T03:59:46\n",
      "Begin sorting into Hashes\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hash Tweet Freq plot\n",
    "function tohashFreq(HashTweetMap)\n",
    "    #return map(j->map(z -> Pair(z[1],map(i -> i[\"timestamp_ms\"],z[2])),j),HashTweetMap)\n",
    "    return  map(i -> mapByKey(z->map(j -> j[\"timestamp_ms\"],z),i),HashTweetMap)\n",
    "end\n",
    "x_source = mapreduce(x->tohashFreq(toHashMap(toDstream(map(y -> JSON.parse(y),readlines(GZip.open(\"../16-06-17/\"*x))),120000))),vcat,readdir(\"../16-06-17\")[1:2])\n",
    "1\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin binning to 2 minutes slots\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2016-06-17T03:49:59\n",
      "windowCount:1\n",
      "Begin sorting into Hashes\n"
     ]
    }
   ],
   "source": [
    "x = toHashMap(toDstream(map(y -> JSON.parse(y),readlines(GZip.open(\"../16-06-17/00-00.json.gz\"))[1:50]),120000));\n",
    "1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## x_pos_data[:,find(x_pos_data[1,:])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(303,)\n",
      "(12863,)\n",
      "1.415071507150715\n"
     ]
    }
   ],
   "source": [
    "println(size(unique(map(x->x[2],trendingHashes))))\n",
    "println(size(map(x->x[2],trendingHashes)))\n",
    "println(size(map(x->x[2],trendingHashes))[1]/size(unique(map(x->x[2],trendingHashes)))[1]/30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin binning to 2 minutes slots\n",
      "2016-06-17T03:49:59\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin sorting into Hashes\n"
     ]
    }
   ],
   "source": [
    "x = mapreduce(x->map(y -> JSON.parse(y),readlines(GZip.open(\"../16-06-17/\"*x))),vcat,readdir(\"../16-06-17\"))\n",
    "y = toHashMap(toDstream(x,1200000))\n",
    "map(j -> map(z -> Pair(z[1],map(i -> i[\"timestamp_ms\"],z[2])),j), y)\n",
    "1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2-element Array{Array{Any,1},1}:\n",
       " Any[\"#KD2DC\"=>Any[\"1466135399659\"],\"#anarchy\"=>Any[\"1466135399657\"],\"#creature\"=>Any[\"1466135399657\"],\"#메이저놀이터추천\\n#메이저놀이터추천\\n사-설-토-토-추-천-사-이-트\\n👔\\n👔\\n👔\\n👔\\n👔\\n🔮🐌🔮\"=>Any[\"1466135399661\"],\"#creatureart\"=>Any[\"1466135399657\"],\"#विलंबित_न्याय_है_अन्याय\"=>Any[\"1466135399664\"],\"#art\"=>Any[\"1466135399657\"],\"#PS4live\"=>Any[\"1466135399657\"],\"#alien\"=>Any[\"1466135399657\"],\"#MTVAWARDSSTAR\"=>Any[\"1466135399659\",\"1466135399657\"],\"#ViajoConAlexTienda\"=>Any[\"1466135399658\"],\"#OrangeIsTheNewBlack?\"=>Any[\"1466135399659\"]]\n",
       " Any[\"#ドッカンバトル\"=>Any[\"1466135400664\"],\"#Game7\"=>Any[\"1466135400661\"],\"#11yearsWithTVXQ\"=>Any[\"1466135400665\"],\"#NBAFinals\"=>Any[\"1466135400662\"],\"#ArjonaEnDibujos\"=>Any[\"1466135400661\"],\"#1/14\\n\\nPor\"=>Any[\"1466135400661\"],\"#KLiteVote\\\"\"=>Any[\"1466135400660\"],\"#ALLin216\"=>Any[\"1466135400660\"],\"#ポートレート\\n#青森県内の被写体さんと繋がりたい\\n#被写体さんと繋がりたい\\n#ファインダー越しの私の世界\\n#雰囲気好きな人RT\"=>Any[\"1466135400660\"],\"#DesafioWarcraft\"=>Any[\"1466135400666\"],\"#AllInCavs\"=>Any[\"1466135400661\"]]                                                "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = y[2][\"#NBAFinals\"]\n",
    "map(j -> map(z -> Pair(z[1],map(i -> i[\"timestamp_ms\"],z[2])),j), y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1466135608963"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using JSON\n",
    "import GZip\n",
    "trendx = mapreduce(x->map(y -> JSON.parse(y),readlines(GZip.open(\"../trend16-06-17/\"*x))),vcat,readdir(\"../trend16-06-17\")[1:6]) #File Readin\n",
    "trendx = mapreduce(i -> mapreduce(j-> Pair(i[\"timestamp_ms\"],j[\"Name\"]),vcat,i[\"Trend\"]),vcat,trendx)\n",
    "trendingHashes = unique(trendx[find(i -> i[2][1] == '#',trendx)])\n",
    "recurrentCount = 5\n",
    "#dt = Dates.datetime2unix(startTime)*1000\n",
    "dt = first(trendingHashes)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "LoadError: type Array has no field val\nwhile loading In[9], in expression starting on line 1",
     "output_type": "error",
     "traceback": [
      "LoadError: type Array has no field val\nwhile loading In[9], in expression starting on line 1",
      ""
     ]
    }
   ],
   "source": [
    "trendingHashes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5-element Array{Any,1}:\n",
       " [5,5,5]   \n",
       " [9,9,9]   \n",
       " [10,10,10]\n",
       " [15,15,15]\n",
       " [20,20,20]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x_store[i][\"\\#OceanMovies\"] for i in collect(2:6)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dict{AbstractString,Any} with 12 entries:\n",
       "  \"#KD2DC\"                 => Any[\"1466135399659\"]\n",
       "  \"#anarchy\"               => Any[\"1466135399657\"]\n",
       "  \"#creature\"              => Any[\"1466135399657\"]\n",
       "  \"#메이저놀이터추천\\n#메… => Any[\"1466135399661\"]\n",
       "  \"#creatureart\"           => Any[\"1466135399657\"]\n",
       "  \"#विलंबित_न्याय_है_अ… => Any[\"1466135399664\"]\n",
       "  \"#art\"                   => Any[\"1466135399657\"]\n",
       "  \"#PS4live\"               => Any[\"1466135399657\"]\n",
       "  \"#alien\"                 => Any[\"1466135399657\"]\n",
       "  \"#MTVAWARDSSTAR\"         => Any[\"1466135399659\",\"1466135399657\"]\n",
       "  \"#ViajoConAlexTienda\"    => Any[\"1466135399658\"]\n",
       "  \"#OrangeIsTheNewBlack?\"  => Any[\"1466135399659\"]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_source[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "true"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Hash Freq analysis\n",
    "function freqAnalaysis(dateString)\n",
    "    println(\"Begin analysis\")\n",
    "    trendx = mapreduce(x->map(y -> JSON.parse(y),readlines(GZip.open(\"Step0_Raw/Json/trend\"*dateString*\"/\"*x))),vcat,readdir(\"Step0_Raw/Json/trend\"*dateString));\n",
    "    trendx = mapreduce(i -> mapreduce(j-> Pair(i[\"timestamp_ms\"],j[\"Name\"]),vcat,i[\"Trend\"]),vcat,trendx)\n",
    "    trendingHashes = unique(trendx[find(i -> i[2][1] == '#',trendx)]);\n",
    "    trendingHashes = reduce(vcat, map(x-> hcat(x[2],x[1]),trendingHashes))\n",
    "    println(\"found all trends\")\n",
    "    trendingHashtags = unique(trendingHashes[:,1]) #Important A list of trending hashes during this 24 hour window.\n",
    "    trendingHashtagsFirstTimestamp = hcat(trendingHashes[map(i->findfirst(trendingHashes[:,1],i),trendingHashtags),:],collect(1:length(trendingHashtags)))\n",
    "    writecsv(\"trendingHashtagsFirstTimestamp.csv\",trendingHashtagsFirstTimestamp)\n",
    "    println(\"Saved trending Hashes and their first time stamp\")\n",
    "    trendingHashtags = vcat(trendingHashtags,[\"#love\";\"#Pokemon\";\"#happy\"]) #Adding non trending hashtags\n",
    "    x_source = mapreduce(x->tohashFreq(toHashMap(toDstream(map(y -> JSON.parse(y),readlines(GZip.open(\"Step0_Raw/Json/\"*dateString*\"/\"*x))),120000))),vcat,readdir(\"Step0_Raw/Json/\"*dateString)[10:15]);\n",
    "    println(\"Begin construct Freq Analysis for the chosen hashtags\")\n",
    "    z = map(k -> Pair(k,map(y-> y[2],mapreduce(x-> x[find(i -> i[1] == k,x)],vcat,x_source))),trendingHashtags);\n",
    "    print(length(find(i -> ! isempty(i[2]),z))) print(\"/\")\n",
    "    print(length(z))\n",
    "    println(\" Empty\")\n",
    "    z = z[find(i -> ! isempty(i[2]),z)];\n",
    "    z = map(k -> Pair(k[1],reduce(vcat,k[2])),z);\n",
    "    sol = mapreduce(z -> hcat(z[1],z[2]),vcat,mapreduce(x->map(i->(i,find(j -> j==x[1],trendingHashtags)[1]),x[2]),vcat ,z));\n",
    "    writecsv(\"HashFreqAnalaysis.csv\",sol)\n",
    "    #Sol is set to chart hash freq \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(x_vector,vectorIndMapping) = toXVectors(x_store);\n",
    "1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18-element Array{Number,1}:\n",
       " 117.0\n",
       " 117.0\n",
       " 186.0\n",
       " 137.0\n",
       " 137.0\n",
       " 215.0\n",
       " 129.0\n",
       " 129.0\n",
       " 193.0\n",
       " 109.0\n",
       " 109.0\n",
       " 158.0\n",
       " 110.0\n",
       " 111.0\n",
       " 160.0\n",
       " 108.0\n",
       " 109.0\n",
       " 157.0"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(x_vector[find(x -> x == 1,y_vector)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.4.0",
   "language": "julia",
   "name": "julia-0.4"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
